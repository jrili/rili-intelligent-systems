{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS280 Programming Assignment 1\n",
    "Naive Bayes Spam Filter\n",
    "<br>\n",
    "Dataset: TREC06 Corpus<br>\n",
    "<br>\n",
    "__IMPORTANT NOTE BEFORE RUNNING:__\n",
    "* Download and unzip __trec06p-cs280.zip__ into a folder named __'trec06p-cs280'__\n",
    "* Move the folder __'trec06p-cs280\\data'__ into the upper folder, e.g.\n",
    ">$ mv trec06p-cs280\\data ..\\data\n",
    "\n",
    "> __EXPECTED DIRECTORY CONTENTS__<br>\n",
    "$ ls <br>\n",
    "PA1_RILI.ipynb<br>\n",
    "trec06p-cs280<br>\n",
    "data<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Classifier Construction and Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parse the documents inthe training set, form the vocabulary V, count their statistics and report the priori probabilities for spam and ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels and data paths for both the train and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_file_path = 'trec06p-cs280\\labels'\n",
    "data_root_path = 'data'\n",
    "train_data_start_path = 'data/000/000'\n",
    "train_data_end_path = 'data/070/299'\n",
    "test_data_start_path = 'data/071/000'\n",
    "test_data_end_path = 'data/126/021'\n",
    "\n",
    "train_labels = []\n",
    "train_data_paths = []\n",
    "test_labels = []\n",
    "test_data_paths = []\n",
    "\n",
    "with open(labels_file_path) as labels_file:\n",
    "    in_test_data_part = False\n",
    "    for line in labels_file:\n",
    "        line_contents = line.strip().split()\n",
    "        if in_test_data_part:\n",
    "            test_labels.append(line_contents[0])\n",
    "            test_data_paths.append(line_contents[1][3:])\n",
    "        else:\n",
    "            train_labels.append(line_contents[0])\n",
    "            train_data_paths.append(line_contents[1][3:])\n",
    "            \n",
    "        if train_data_end_path in line_contents[1]:\n",
    "            in_test_data_part = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a few sanity checks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_labels), len(train_data_paths))\n",
    "print(train_labels[-1], train_data_paths[-1])\n",
    "print(len(test_labels), len(test_data_paths))\n",
    "print(test_labels[-1], test_data_paths[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Priors\n",
    "The Prior probabilities, P(w=H) and P(w=S) are thus computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_ham = train_labels.count('ham') / len(train_labels)\n",
    "prior_spam = train_labels.count('spam') / len(train_labels)\n",
    "\n",
    "print('P(w=H) = ', prior_ham)\n",
    "print('P(w=S) = ', prior_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a function that would give us the \"words\" (as defined in the specifications) given a line/string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_line(line):\n",
    "    pattern = re.compile(r'(?<=\\s)[a-zA-Z]+[\\-\\']?[\\.|\\,|\\s]')\n",
    "    words = pattern.findall(line)\n",
    "    for word_index, word in enumerate(words):\n",
    "        words[word_index] = word.strip(\" \\t\\n.,-'\").lower()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let us define some *stop words* -- words that are so commonly used that they become insignificant and meaningless.<br>\n",
    "(Taken from https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \n",
    "\"again\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",    \n",
    "\"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"as\", \"at\", \"be\", \"became\", \"because\", \"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"behind\", \"being\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\",\"can\", \"cannot\", \"cant\", \"could\", \"couldnt\", \"de\", \"describe\", \"do\", \"done\", \"each\", \"eg\", \"either\", \"else\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"find\",\"for\",\"found\", \"four\", \"from\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \"ie\", \"if\", \"in\", \"indeed\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\",\"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\",\"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"she\", \"should\",\"since\", \"sincere\",\"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"take\",\"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\",\n",
    "\"this\", \"those\", \"though\", \"through\", \"throughout\",\n",
    "\"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\n",
    "\"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "\"very\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "\"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "\"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \n",
    "\"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "\"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let us define a function that parses all the training data files to construct our vocabulary. <br>\n",
    "Our vocabulary comes in the form of a dictionary with the words as the key and their corresponding indices as their values. This will come in useful later on when constructing the vocabulary vectors for the input documents to our spam filter.<br><br>\n",
    "Note that only words less than a set maximum word length are included in an attempt to reduce the strain on the run-time memory. <br>\n",
    "> Note that the average length of words in the English dictionary is around ~5. (http://norvig.com/mayzner.html) So here we set it to a reasonable multiple of the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "#from playsound import playsound\n",
    "\n",
    "def get_vocab_with_indices():\n",
    "    labels_file_path = 'trec06p-cs280\\labels'\n",
    "    data_path = 'data'\n",
    "    MAX_WORD_LEN = 10\n",
    "\n",
    "    vocab_indices = {}\n",
    "    current_vocab_index = 0\n",
    "\n",
    "    for data_path_index, data_path in enumerate(train_data_paths):\n",
    "        with open(data_path, 'r', errors='ignore') as file:\n",
    "            print('Parsing file %d of %d for vocabulary:'%(data_path_index, len(train_data_paths)))\n",
    "            for line in file:\n",
    "                words = get_words_from_line(line)\n",
    "                for word in words:\n",
    "                    if word not in stop_words and len(word) <= MAX_WORD_LEN:\n",
    "                        if (word not in vocab_indices):\n",
    "                            vocab_indices[word] = current_vocab_index\n",
    "                            current_vocab_index += 1\n",
    "\n",
    "    #playsound('Victory.mp3')\n",
    "\n",
    "    return vocab_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can call the above function like so:\n",
    ">Optional: This may take a while. Uncomment the playsound() line and its necessary imports to play a sound when the processing is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = get_vocab_with_indices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of vocabulary words: ', len(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the vector of likelihoods per document class\n",
    "Now that we have our vocabulary, let us now define a method that returns a vector of word counts for each input document from our training data of a specified class (i.e. \"spam\" or \"ham\"). Additionally, it also returns the number of documents processed that matches the specified label. <br>\n",
    "\n",
    "This is done by first creating a word-presence matrix (__'all_documents'__): The rows correspond to each word in our vocabulary while the columns correspond to each training data document labeled with __'document_class'__. If a certain word is present in a certain document, then the corresponding element in the matrix will be marked with a '1'.\n",
    "\n",
    "Afterwards, the desired word counts can be obtained by simply summing along the rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_word_counts(document_class, vocab_indices, train_data_paths, train_labels):\n",
    "    print('vocab_indices length: ', len(vocab_indices))\n",
    "    if document_class not in train_labels:\n",
    "        return None\n",
    "    class_doc_index = 0\n",
    "    all_documents = np.zeros((len(vocab_indices),train_labels.count(document_class)), dtype=np.float64)\n",
    "    print('all_documents shape: ', all_documents.shape)\n",
    "    for doc_index, data_path in enumerate(train_data_paths):\n",
    "        print('Processing document file %d of %d to vocabulary vector:'%(doc_index, len(train_data_paths)))\n",
    "        if (train_labels[doc_index] != document_class):\n",
    "            continue\n",
    "        with open(data_path, 'r', errors='ignore') as file:\n",
    "            for line in file:\n",
    "                words = get_words_from_line(line)\n",
    "                for word in words:\n",
    "                    if word in vocab_indices:\n",
    "                        if all_documents[vocab_indices[word],class_doc_index] == 0:\n",
    "                            #print('\\tFound: word \"%s\" (index %d) in %s doc_index %d path %s'%(word, vocab_indices[word], document_class, class_doc_index, data_path))\n",
    "                            all_documents[vocab_indices[word],class_doc_index] = 1\n",
    "                        else:\n",
    "                            continue\n",
    "        class_doc_index += 1\n",
    "    word_counts = all_documents.sum(axis=1)\n",
    "    #playsound('Victory.mp3')\n",
    "    return word_counts, all_documents.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply call the function for both 'ham' and 'spam' classes and store the results.<br>\n",
    ">Optional: This may take a while. Uncomment the playsound() line and its necessary imports to play a sound when the processing is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_ham, num_ham_train_docs = get_word_counts('ham', V, train_data_paths, train_labels)\n",
    "word_counts_spam, num_spam_train_docs = get_word_counts('spam', V, train_data_paths, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us then define a function that returns a vector of likelihood probabilities or the probability that a specific word is present in a document given it has class __w__, e.g. *P(word_is_present | w)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihoods_for(word_counts, total_num_class_docs):\n",
    "    return word_counts/total_num_class_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods_ham = get_likelihoods_for(word_counts_ham, num_ham_train_docs)\n",
    "likelihoods_spam = get_likelihoods_for(word_counts_spam, num_spam_train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the likelihoods vectors are used: Let's say we want to know the likelihood probability for a certain word. We fetch first the corresponding index value in V using the word as a key, then simply use the result as an index for the likelihood vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'tears'\n",
    "likelihoods_ham[V[word]], likelihoods_spam[V[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construct and train a Naive Bayesian Classifier from the count statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can implement our classifier, let us first define a function that converts an input document from a specified __data_path__ into a word-presence vector corresponding to our vocabulary. Similar to our word-presence matrix earlier, each row corresponds to our vocabulary words and a value of 1 means the corresponding vocabulary word is present in the document, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_vocab_vector(data_path, vocab_indices):\n",
    "    vocab_vector = np.zeros((len(vocab_indices)), dtype=np.int8)\n",
    "    print('Processing document file %s to vocabulary vector:'%(data_path))\n",
    "    with open(data_path, 'r', errors='ignore') as file:\n",
    "        for line in file:\n",
    "            words = get_words_from_line(line)\n",
    "            for word in words:\n",
    "                if word in vocab_indices:\n",
    "                    if vocab_vector[vocab_indices[word]] == 0:\n",
    "                        vocab_vector[vocab_indices[word]] = 1\n",
    "                    else:\n",
    "                        continue\n",
    "    return vocab_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per Bayes' rule, what we need to compute for are the conditional probabilities for each document class (i.e. P(ham|w1, ..., wn) and P(spam|w1, ..., wn) ) which will then be compared. The classifier would then output a prediction corresponding to the class with the higher probability.<br>\n",
    "<br>\n",
    "The function __calculate_probability()__ does this computation given the document's word-presence vocabulary vector. It uses the function __calculate_sum_of_log_likelihoods__ to get the sum of the logarithm of the likelihood probabilities for each word, then converts it back to normal (non-logarithmic) space to be compared.<br>\n",
    "<br><br>\n",
    "To compute the sum of the logarithms of each likelihood, the function __calculate_sum_of_log_likelihoods()__ makes use of the inverse of the document vocabulary vector (__doc_vocab_vector_inverse__) and also the inverse of the likelihood probabilities (__likelihoods_inverse__).<br>\n",
    "* __doc_vocab_vector__: a single training document input converted to word-presence vocabulary vector\n",
    "* __doc_vocab_vector_inverse__: Each element corresponds to the opposite (i.e. 0 if 1; 1 if 0) of each value in the converted word-presence vocabulary vector (__doc_vocab_vector__)<br>\n",
    "* __likelihoods__: contains the likelihood probabilities previously obtained,e.g. P(w1|__doc_class__), ..., P(wn|__doc_class__)\n",
    "* __likelihoods_inverse__: Each element here corresponds to the probability of the inverse of the likelihood probabilities. (i.e. 1 - likelihood)\n",
    "The probabilities to be multiplied (or in this case, taken the logarithm of and then summed up)are then computed as follows as __doc_likelihoods__:\n",
    "\n",
    "doc_likelihoods = doc_vocab_vector$*$likelihoods + doc_ vocab_ vector_ inverse$*$likelihoods_ inverse\n",
    "\n",
    "* if word is NOT present in the document (e.g. __doc_vocab_vector[i]__=1 and __doc_vocab_vector_inverse[i]__=0), then use the value in __likelihoods[i]__.\n",
    "* if word is present in the document (e.g. __doc_vocab_vector[i]__=0 and __doc_vocab_vector_inverse[i]__=1), then use the value in __likelihoods_inverse[i]__.<br>\n",
    "<br>\n",
    "Also note that the words with zero likelihood in __doc_likelihoods__ were excluded when taking the sum of logarithms. Taking the logarithm of 0 results to -inf which results to an overflow exception  when summed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, e\n",
    "\n",
    "def predict_doc_class(doc_path, vocab_indices, likelihoods_ham, likelihoods_spam, prior_ham, prior_spam):\n",
    " \n",
    "\n",
    "    def calculate_probability(doc_vocab_vector, doc_class, vocab_indices, likelihoods_ham, likelihoods_spam, prior_ham, prior_spam):\n",
    "        doc_vocab_vector_inverse = np.bitwise_xor(doc_vocab_vector.astype(np.int8), np.ones(doc_vocab_vector.shape, dtype=np.int8))\n",
    "        doc_vocab_vector_inverse = doc_vocab_vector_inverse.astype(float)\n",
    "\n",
    "        def calculate_sum_of_log_likelihoods(doc_vocab_vector, doc_vocab_vector_inverse, likelihoods):\n",
    "            \n",
    "            likelihoods_inverse = np.ones(likelihoods.shape, dtype=np.float64)-likelihoods\n",
    "            \n",
    "            doc_likelihoods = doc_vocab_vector*likelihoods + doc_vocab_vector_inverse*likelihoods_inverse\n",
    "            \n",
    "            #exclude zero-valued doc_likelihood elements to avoid overflow!\n",
    "            logsum_likelihoods = np.sum(np.log(doc_likelihoods[doc_likelihoods != 0]))\n",
    "            return logsum_likelihoods\n",
    "\n",
    "        logsum_likelihoods_spam = calculate_sum_of_log_likelihoods(doc_vocab_vector, doc_vocab_vector_inverse, likelihoods_spam)\n",
    "        logsum_likelihoods_ham = calculate_sum_of_log_likelihoods(doc_vocab_vector, doc_vocab_vector_inverse, likelihoods_ham)\n",
    "        logsum_likelihoods = 0\n",
    "        logsum_likelihoods_other = 0\n",
    "        prior = 0\n",
    "        prior_other = 0\n",
    "        if doc_class == 'ham':\n",
    "            logsum_likelihoods = logsum_likelihoods_ham\n",
    "            logsum_likelihoods_other = logsum_likelihoods_spam\n",
    "            prior = prior_ham\n",
    "            prior_other = prior_spam\n",
    "        else:\n",
    "            logsum_likelihoods = logsum_likelihoods_spam\n",
    "            logsum_likelihoods_other = logsum_likelihoods_ham\n",
    "            prior = prior_spam\n",
    "            prior_other = prior_ham\n",
    "\n",
    "        probability_log = logsum_likelihoods + log(prior)\n",
    "        probability_log_other = logsum_likelihoods_other + log(prior_other)\n",
    "        \n",
    "        conditional_probability = (e**probability_log) / ((e**probability_log) + (e**probability_log_other))\n",
    "\n",
    "        return conditional_probability\n",
    "    \n",
    "    doc_vocab_vector = doc_to_vocab_vector(doc_path, V)\n",
    "    doc_vocab_vector = doc_vocab_vector.astype(float)\n",
    "    predicted_class = 'ham'\n",
    "    probability_ham = calculate_probability(doc_vocab_vector, 'ham', V, likelihoods_ham, likelihoods_spam, prior_ham, prior_spam)\n",
    "    probability_spam = calculate_probability(doc_vocab_vector, 'spam', V, likelihoods_ham, likelihoods_spam, prior_ham, prior_spam)\n",
    "    \n",
    "    print('probability_ham = %12.10f ; probability_spam = %12.10f'%(probability_ham, probability_spam))\n",
    "    \n",
    "    if probability_spam > probability_ham:\n",
    "        predicted_class = 'spam'\n",
    "    return predicted_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement the code for classifying an unknown message and try it on the testset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, there's really nothing left to do but to call the function we declared previously, __predict_doc_class()__, for all the files in __test_data_paths__ with their corresponding labels __test_labels__. <br>\n",
    "<br>\n",
    "And this we did, enclosing it in a function __predict_doc_classses()__ which can be useful later on.\n",
    ">Optional: This may take a while. Uncomment the playsound() line and its necessary imports to play a sound when the processing is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_doc_classes(test_data_paths, test_labels, likelihoods_ham, likelihoods_spam, prior_ham, prior_spam):\n",
    "    predicted_labels = []\n",
    "    for data_path_index, data_path in enumerate(test_data_paths):\n",
    "        print('\\nProcessing data_path_index: %d'%(data_path_index))\n",
    "        predicted_labels.append(predict_doc_class(data_path, V, likelihoods_ham, likelihoods_spam, prior_ham, prior_spam))\n",
    "        print('Predicted: %s ; Correct: %s'%(predicted_labels[data_path_index], test_labels[data_path_index]))\n",
    "\n",
    "    #playsound('Victory.mp3')\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "predicted_labels = predict_doc_classes(test_data_paths, test_labels,\n",
    "                                       likelihoods_ham, likelihoods_spam,\n",
    "                                       prior_ham, prior_spam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write a function that computes the precision and recall measures.\n",
    "* Precision = TP/(TP + FP)<br>\n",
    "* Recall = TP/(TP + FN)<br>\n",
    "<br>\n",
    "* TP: num of SPAM messages classified as SPAM\n",
    "* TN: num of HAM messages classified as HAM\n",
    "* FP: num of HAM messages misclassified as SPAM\n",
    "* FN: num of SPAM messages misclassified as HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(true_labels, predicted_labels):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for index in range(len(true_labels)):\n",
    "        TP += int(true_labels[index] == 'spam')*int(predicted_labels[index] == 'spam')\n",
    "        TN += int(true_labels[index] == 'ham')*int(predicted_labels[index] == 'ham')\n",
    "        FP += int(true_labels[index] == 'ham')*int(predicted_labels[index] == 'spam')\n",
    "        FN += int(true_labels[index] == 'spam')*int(predicted_labels[index] == 'ham')\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    return precision, recall    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = calculate_scores(test_labels, predicted_labels)\n",
    "print('Test Results without smoothing:')\n",
    "print('\\tPrecision=', precision)\n",
    "print('\\tRecall=', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Lambda Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write another function that uses Lambda smoothing\n",
    "To apply Lambda smoothing, we need to modify the formula for getting the likelihood probabilities:\n",
    "* An extra __lmbda__ term in the numerator\n",
    "* and an extra __lmbda__ * |V| in the denominator\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "To do this, we take the __predict_doc_classes()__ function above and internally modify the __get_likelihoods_for()__ function:\n",
    "\n",
    ">Optional: This may take a while. Uncomment the playsound() line and its necessary imports to play a sound when the processing is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_doc_classes_withsmoothing(test_data_paths, test_labels, vocab_indices,\n",
    "                                      prior_ham, prior_spam,\n",
    "                                      word_counts_ham, word_counts_spam,\n",
    "                                      num_ham_train_docs, num_spam_train_docs,\n",
    "                                      lmbda):\n",
    "    predicted_labels = []\n",
    "    \n",
    "    def get_likelihoods_for(word_counts, total_num_class_docs, vocab_indices, lmbda=1.0):\n",
    "        return (word_counts+lmbda)/(total_num_class_docs+lmbda*len(vocab_indices))\n",
    "    \n",
    "    likelihoods_ham = get_likelihoods_for(word_counts_ham, num_ham_train_docs, vocab_indices, lmbda)\n",
    "    likelihoods_spam = get_likelihoods_for(word_counts_spam, num_spam_train_docs, vocab_indices, lmbda)\n",
    "    \n",
    "    for data_path_index, data_path in enumerate(test_data_paths):\n",
    "        print('\\nProcessing data_path_index: %d'%(data_path_index))\n",
    "        predicted_labels.append(predict_doc_class(data_path, vocab_indices, likelihoods_ham, likelihoods_spam, prior_ham, prior_spam))\n",
    "        print('Path: %s ; Predicted: %s ; Correct: %s'%(data_path, predicted_labels[data_path_index], test_labels[data_path_index]))\n",
    "\n",
    "    #playsound('Victory.mp3')\n",
    "    return predicted_labels, likelihoods_ham, likelihoods_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use Lambda smoothing for 5 different values of lambda (2.0, 1.0, 0.5, 0.1, 0.005) and print the precision and recall for these values. What value of lambda yields the best precision and recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a new function similar to __calculate_scores()__ defined previously added with informative prints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores_withsmoothing(test_labels, predicted_labels, lmbda):\n",
    "    precision, recall = calculate_scores(test_labels, predicted_labels)\n",
    "    print('Test Results for Smoothing with lambda=', lmbda)\n",
    "    print('Precision=', precision)\n",
    "    print('Recall=', recall)\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we call the functions previously defined for all the lambda values listed and gather their respective precision and recall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [2.0, 1.0, 0.5, 0.1, 0.005]\n",
    "precision_scores = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "recall_scores = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "for index, lmbda in enumerate(lambdas):\n",
    "    predicted_labelswithsmoothing = []\n",
    "    predicted_labelswithsmoothing, likelihoods_h, likelihoods_s = predict_doc_classes_withsmoothing(test_data_paths, test_labels,\n",
    "                                                                      V, prior_ham, prior_spam,\n",
    "                                                                      word_counts_ham, word_counts_spam,\n",
    "                                                                      num_ham_train_docs, num_spam_train_docs,\n",
    "                                                                      lmbda)\n",
    "    \n",
    "    precision, recall = calculate_scores_withsmoothing(test_labels, predicted_labelswithsmoothing, lmbda)\n",
    "    precision_scores[index] = precision\n",
    "    recall_scores[index] = recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x, lmbda in enumerate(lambdas):\n",
    "    print('For lambda=%5.3f: precision=%10.8f ; recall=%10.8f'%(lmbda, precision_scores[x], recall_scores[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Improving your Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find a way to identify the most informative words for spam filtering. Using the best value of lambda found above, print the top 200 informative words for spam and ham messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to be creating a new vocabulary, let us set the vocabulary dictionary __V__ to an empty dict to free up some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V={}\n",
    "word_counts_ham=[]\n",
    "word_counts_spam=[]\n",
    "likelihoods_ham=[]\n",
    "likelihoods_spam=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the reference paper by Hovold, the frequency of occurence of the words in the training data should be considered when building the vocabulary for the filter. More specifically:\n",
    "* Exclude the words with less than 3 occurences in the whole training data set\n",
    "* Exclude 100 to 200 most frequently occuring words in the whole training data set<br>\n",
    "\n",
    "\n",
    "This is implemented in the function __get_vocab_with_freq()__. The training data is parsed again to list down the words similar to what was done in __get_vocab_with_indices()__ but this time taking note of the number of times each word is encountered and is stored in the dict __vocab_freq__. The dict is then arranged by order of descending word frequency (i.e. from highest to lowest)\n",
    "As suggested by the referenced paper, (1) words whose frequencies are less than __MIN_WORDFREQ__ are not included in the vocabulary and (2)the top __NUM_MOST_FREQUENT_TO_REMOVE__ most frequently occuring words are not included in the vocabulary.<br>\n",
    "<br>\n",
    "Also, notice that we imposed a limit on the length of words to include in the vocabulary. This is because, without doing so, the size of this jupyter notebook soars to ~100MB. This size causes the notebook to function slowly and then eventually crash. This is avoided by imposing a length limit on the vocabulary.<br>\n",
    "<br>\n",
    ">Optional: This may take a while. Uncomment the playsound() line and its necessary imports to play a sound when the processing is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from playsound import playsound\n",
    "\n",
    "def get_vocab_with_freq(train_data_paths):\n",
    "    \n",
    "    MIN_WORDFREQ = 3\n",
    "    NUM_MOST_FREQUENT_TO_REMOVE = 150\n",
    "    MAX_VOCAB_SIZE = 200\n",
    "    MAX_WORD_LEN = 10\n",
    "\n",
    "    vocab_freq = {}\n",
    "    current_vocab_index = 0\n",
    "\n",
    "    for data_path_index, data_path in enumerate(train_data_paths):\n",
    "        with open(data_path, 'r', errors='ignore') as file:\n",
    "            print('Parsing file %d of %d for vocabulary:'%(data_path_index, len(train_data_paths)))\n",
    "            for line in file:\n",
    "                words = get_words_from_line(line)\n",
    "                for word in words:\n",
    "                    if word not in stop_words and (len(word) <= MAX_WORD_LEN):\n",
    "                        if (word not in vocab_freq):\n",
    "                            vocab_freq[word] = 1\n",
    "                        else:\n",
    "                            vocab_freq[word] += 1\n",
    "    \n",
    "    # Remove words with frequency < MIN_WORDFREQ\n",
    "    vocab_freq = {word: freq for word, freq in vocab_freq.items() if freq >= MIN_WORDFREQ}\n",
    "    \n",
    "    # Then sort the dictionary by frequency, descending order\n",
    "    vocab_freq = sorted(vocab_freq.items(), key = itemgetter(1))\n",
    "    vocab_freq.reverse()\n",
    "    \n",
    "    # Finally, we get only the first 200 from the resulting list\n",
    "    vocab_freq = vocab_freq[NUM_MOST_FREQUENT_TO_REMOVE+1:]\n",
    "    vocab_freq = dict(vocab_freq[:MAX_VOCAB_SIZE])\n",
    "    \n",
    "    #playsound('Victory.mp3')\n",
    "\n",
    "    return vocab_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_freq = get_vocab_with_freq(train_data_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we already have our vocabulary in __vocab_freq__ along with each word's frequency. We don't need the frequencies anymore, so let us define a function __get_vocab_indices()__to get their respective indices instead, similar to the output of __get_vocab_with_indices()__ previously declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_indices(vocab_freq):\n",
    "    vocab_indices = {}\n",
    "    current_vocab_index = 0\n",
    "    for index, word in enumerate(vocab_freq):\n",
    "        vocab_indices[word] = current_vocab_index\n",
    "        current_vocab_index += 1\n",
    "    return vocab_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = get_vocab_indices(vocab_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate the precision and recall of your classifier using this smaller vocabulary of 200 words.\n",
    "<br>\n",
    "Now that we have our brand new, smaller vocabulary, let us repeat the steps previously outlined to train, test, and evaluate our Naive Bayes Spam Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_ham, num_ham_train_docs = get_word_counts('ham', V, train_data_paths, train_labels)\n",
    "word_counts_spam, num_spam_train_docs = get_word_counts('spam', V, train_data_paths, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda = 0.5\n",
    "\n",
    "predicted_labelswithsmoothing = []\n",
    "predicted_labelswithsmoothing, likelihood_ham, likelihood_spam = predict_doc_classes_withsmoothing(test_data_paths, test_labels,\n",
    "                                            V, prior_ham, prior_spam,\n",
    "                                            word_counts_ham, word_counts_spam,\n",
    "                                            num_ham_train_docs, num_spam_train_docs,\n",
    "                                            lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = calculate_scores_withsmoothing(test_labels, predicted_labelswithsmoothing, lmbda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
