{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS280 Programming Assignment 2\n",
    "__Implementing the Backpropagation Algorithm__<br>\n",
    "<br>\n",
    "Compiler: Python 3.6.5<br>\n",
    "OS: Windows 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification via ANN\n",
    "Let's implement an artificial neural network with:\n",
    "* four layers (2 hidden layers)\n",
    "* MSE as the error function\n",
    "* Sigmoid function as the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training and validation data\n",
    "The training and validation data used here are processed from __data.csv__ and __data_labels.csv__ using the scripts in the notebook __Handling_Imbalanced_Data.ipynb__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('training_set.csv', delimiter=',')\n",
    "train_labels = np.genfromtxt('training_labels.csv', delimiter=',')\n",
    "\n",
    "validation_data = np.genfromtxt('validation_set.csv', delimiter=',')\n",
    "validation_labels = np.genfromtxt('validation_labels.csv', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Convert labels to one-hot vector format\n",
    "For classification tasks such as this, it is convenient to represent the target labels as a one-hot vector. In this case, the data contains 8 unique target labels, so the one-hot vector encoded labels would look like:<br>\n",
    "> Class Label &ensp;&ensp;&ensp; One-Hot-Vector Encoded Format<br>\n",
    "> 1 &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;[1 0 0 0 0 0 0 0]<br>\n",
    "> 2 &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;[0 1 0 0 0 0 0 0]<br>\n",
    "> ....... <br>\n",
    "> 7 &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;[0 0 0 0 0 0 1 0]<br>\n",
    "> 8 &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;[0 0 0 0 0 0 0 1]<br>\n",
    "\n",
    "This is so that the predictions of our model would come out as a vector of probability/confidence values ranging from 0 to 1 for each unique class label.<br>\n",
    "<br>\n",
    "To that end, we define two utility functions for converting from one format to another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_onehotvector(labels):\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "    onehotvector = np.zeros((len(labels), len(unique_labels)))\n",
    "    for index, label in enumerate(labels):\n",
    "        onehotvector[int(index), int(label)-1] = int(1)\n",
    "    return onehotvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotvectors_to_labels(onehotvectors):\n",
    "    labels = np.zeros(onehotvectors.shape[0])\n",
    "    for index, onehotvector in enumerate(onehotvectors):\n",
    "        target_label = np.argwhere(onehotvector/np.max(onehotvector) == 1)\n",
    "        labels[index] = target_label + 1 # +1 because there is no class 0\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".... And use them to convert our labels to one-hot-vector format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = labels_to_onehotvector(train_labels)\n",
    "validation_labels = labels_to_onehotvector(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, train_labels.shape, validation_data.shape, validation_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Neural Network\n",
    "We define a couple of functions to be used for defining, training, and using the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### init_neurons()\n",
    "For allocating the input and output layers and for initializing the weight and bias matrices for the hidden layers randomly using a uniform probability distribution from (-INIT_RANGE) to (+INIT_RANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_neurons(num_input, num_hidden1_neurons, num_hidden2_neurons, num_output):\n",
    "    \n",
    "    INIT_RANGE = 0.1\n",
    "\n",
    "    x_in = np.zeros((num_input, 1))\n",
    "\n",
    "    w_h1 = np.random.uniform(low=-INIT_RANGE, high=INIT_RANGE, size=(num_hidden1_neurons, num_input))\n",
    "    b_h1 = np.random.uniform(low=-INIT_RANGE, high=INIT_RANGE, size=(num_hidden1_neurons, 1))\n",
    "\n",
    "    w_h2 = np.random.uniform(low=-INIT_RANGE, high=INIT_RANGE, size=(num_hidden2_neurons, num_hidden1_neurons))\n",
    "    b_h2 = np.random.uniform(low=-INIT_RANGE, high=INIT_RANGE, size=(num_hidden2_neurons, 1))\n",
    "\n",
    "    w_out = np.random.uniform(low=-INIT_RANGE, high=INIT_RANGE, size=(num_output, num_hidden2_neurons))\n",
    "    b_out = np.random.uniform(low=-INIT_RANGE, high=INIT_RANGE, size=(num_output, 1))\n",
    "\n",
    "    d_out = np.zeros((num_output, 1))\n",
    "    \n",
    "    return x_in, w_h1, b_h1, w_h2, b_h2, w_out, b_out, d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict()\n",
    "Perform forward-pass for a single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_data, w_h1, b_h1, w_h2, b_h2, w_out, b_out):\n",
    "    ## HIDDEN LAYER 1\n",
    "    v_h1 = np.dot(w_h1,input_data) + b_h1\n",
    "    y_h1 = 1/(1 + np.exp(-v_h1))\n",
    "    ## HIDDEN LAYER 2\n",
    "    v_h2 = np.dot(w_h2, y_h1) + b_h2\n",
    "    y_h2 = 1/(1 + np.exp(-v_h2))            \n",
    "    ## OUTPUT LAYER\n",
    "    v_out = np.dot(w_out, y_h2) + b_out\n",
    "    out = 1/(1 + np.exp(-v_out))\n",
    "    \n",
    "    return out, y_h1, y_h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict_batch()\n",
    "Perform forward-pass for a batch of data points and subsequently returns the label predictions.<br>\n",
    "Also computes the total prediction error if *labels* is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(batch_data, w_h1, b_h1, w_h2, b_h2, w_out, b_out, labels=None):\n",
    "    # initialize the total error\n",
    "    total_error = 0\n",
    "    # set the number of classes to the number of neurons in the output layer\n",
    "    num_classes = b_out.shape[0]\n",
    "    onehotvector_predictions = np.zeros((batch_data.shape[0], num_classes))\n",
    "    for index, data in enumerate(batch_data):\n",
    "        error = 0\n",
    "        x_in = batch_data[index].reshape(-1, 1)\n",
    "            \n",
    "        out, dummy1, dummy2 = predict(x_in, w_h1, b_h1, w_h2, b_h2, w_out, b_out)\n",
    "\n",
    "        onehotvector_predictions[index] = out.reshape(-1,)\n",
    "        \n",
    "        if labels is not None:\n",
    "            d_out = labels[index].reshape(-1, 1)\n",
    "            error = d_out - out\n",
    "            total_error = total_error + (np.sum(error*error))/(error.shape[0])\n",
    "    total_error *= (1/batch_data.shape[0])\n",
    "    predictions = onehotvectors_to_labels(onehotvector_predictions)\n",
    "    \n",
    "    return total_error, predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ann_fit()\n",
    "Train the artificial neural network with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_fit(train_data, train_labels, num_input, num_hidden1_neurons, num_hidden2_neurons, num_output,\n",
    "            LR=0.1, validation_period=10, validation_data=None, validation_labels=None):\n",
    "    \n",
    "    # Take note of the start time of training for training duration measurement purposes\n",
    "    train_time_start = time.time()\n",
    "    report_time_start = time.time()\n",
    "    \n",
    "    # Print error report every __ epochs\n",
    "    ERR_REPORT_PERIOD = 100 \n",
    "    \n",
    "    # Stop training if the current total training error goes below this value\n",
    "    ERR_TERMINATION_COND = 0.001\n",
    "    \n",
    "    \n",
    "    # Initialize the input, output, and hidden layer neurons (i.e. their weights and biases matrices)\n",
    "    x_in, w_h1, b_h1, w_h2, b_h2, w_out, b_out, d_out = init_neurons(num_input,\n",
    "                                                                     num_hidden1_neurons,\n",
    "                                                                     num_hidden2_neurons,\n",
    "                                                                     num_output)\n",
    "    # Initialize the error vectors\n",
    "    total_error = np.zeros((MAX_EPOCH, 1))\n",
    "    total_validation_error = np.zeros((int(MAX_EPOCH/VALIDATION_PERIOD), 1))\n",
    "    \n",
    "    # Initialize the epoch value at which training will have ended\n",
    "    training_ending_epoch = MAX_EPOCH-1\n",
    "    \n",
    "    # Generate a vector for the epoch numbers\n",
    "    epochs = range(0, MAX_EPOCH)\n",
    "    \n",
    "    for epoch_index in epochs:\n",
    "        train_indices = np.random.permutation(train_data.shape[0])\n",
    "        for train_index in train_indices:\n",
    "            \n",
    "            # READ DATA\n",
    "            x_in = train_data[train_index].reshape(-1, 1)\n",
    "            d_out = train_labels[train_index].reshape(-1, 1)\n",
    "\n",
    "            ##### FORWARD PASS #####\n",
    "            out, y_h1, y_h2 = predict(x_in, w_h1, b_h1, w_h2, b_h2, w_out, b_out)\n",
    "            \n",
    "            ##### BACK PROPAGATION #####\n",
    "            error = d_out - out\n",
    "            delta_out = error*out*(1-out)\n",
    "            delta_h2 = (y_h2*(1-y_h2))*(np.dot(np.transpose(w_out),delta_out))\n",
    "            delta_h1 = (y_h1*(1-y_h1))*(np.dot(np.transpose(w_h2), delta_h2))\n",
    "\n",
    "            ## Update the weights and biases\n",
    "            w_out = w_out + LR*delta_out*np.transpose(y_h2)\n",
    "            b_out = b_out + LR*delta_out\n",
    "            \n",
    "            w_h2 = w_h2 + LR*delta_h2*np.transpose(y_h1)\n",
    "            b_h2 = b_h2 + LR*delta_h2\n",
    "            \n",
    "            w_h1 = w_h1 + LR*delta_h1*np.transpose(x_in)\n",
    "            b_h1 = b_h1 + LR*delta_h1\n",
    "            \n",
    "            # Update the partial total error for the whole epoch with each training data processed\n",
    "            total_error[epoch_index] = total_error[epoch_index] + (np.sum(error*error))/error.shape[0]\n",
    "            \n",
    "        # .... then take the average for all training data samples processed (Mean Square Error)\n",
    "        total_error[epoch_index] *= (1/train_data.shape[0])\n",
    "        \n",
    "        # Process the validation data set according to the validation period set\n",
    "        current_valerror_index = int(epoch_index/validation_period)\n",
    "        if validation_data is not None and epoch_index%validation_period == 0:\n",
    "            total_validation_error[current_valerror_index], _ = predict_batch(validation_data,\n",
    "                                                                             w_h1, b_h1, w_h2,\n",
    "                                                                             b_h2, w_out, b_out, validation_labels)\n",
    "        \n",
    "        # Print an error report according to the error reporting period\n",
    "        if epoch_index % ERR_REPORT_PERIOD == 0:\n",
    "            report_time_end = time.time()\n",
    "            print('\\nEPOCH %d (duration: %3.4f seconds)'%(epoch_index, report_time_end - report_time_start))\n",
    "            print('\\ttraining error=%10.12f'%(total_error[epoch_index]))\n",
    "            if validation_data is not None:\n",
    "                print('\\tvalidation_error=%10.12f'% (total_validation_error[current_valerror_index]))        \n",
    "            report_time_start = time.time()\n",
    "        \n",
    "\n",
    "        # Update the learning rate LR as the training error gets closer to the target\n",
    "        if LR <= 0.00001:\n",
    "            pass\n",
    "        elif total_error[epoch_index] > 0.006 and total_error[epoch_index] < 0.01:\n",
    "            LR = 0.05\n",
    "        elif total_error[epoch_index] > 0.002 and total_error[epoch_index] < 0.006:\n",
    "            LR = LR - 0.0001\n",
    "        elif total_error[epoch_index]< 0.002:\n",
    "            LR = 0.02\n",
    "        \n",
    "        # Check conditions for early stopping.\n",
    "        # If stopping would be early, compute the validation error using the latest weights and biases\n",
    "        if (total_error[epoch_index] < ERR_TERMINATION_COND):\n",
    "            training_ending_epoch = epoch_index\n",
    "            current_valerror_index += 1\n",
    "            total_validation_error[current_valerror_index], _ = predict_batch(validation_data,\n",
    "                                                                             w_h1, b_h1, w_h2,\n",
    "                                                                             b_h2, w_out, b_out, validation_labels)\n",
    "            break\n",
    "\n",
    "    train_time_end = time.time()\n",
    "    \n",
    "    print('\\n\\n--\\nTRAINING ENDED AT EPOCH %d (%3.3f s) WITH training_error=%10.12f'%(training_ending_epoch,\n",
    "                                                                                      train_time_end-train_time_start,\n",
    "                                                                                      total_error[training_ending_epoch]))\n",
    "    print('\\nTRAINING ENDED AT EPOCH %d WITH validation_error=%10.12f'%(training_ending_epoch,\n",
    "                                                                                      total_validation_error[current_valerror_index]))\n",
    "    \n",
    "    return w_h1, b_h1, w_h2, b_h2, w_out, b_out, total_error, total_validation_error, epochs, training_ending_epoch\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of neurons per layer\n",
    "NUM_INPUT = train_data.shape[1]\n",
    "NUM_HIDDEN1_NEURONS = 11\n",
    "NUM_HIDDEN2_NEURONS = 9\n",
    "NUM_OUTPUT = train_labels.shape[1]\n",
    "\n",
    "# Initial Learning Rate\n",
    "LR = 0.1\n",
    "\n",
    "# Define how often (in number of epochs) the validation data should be tested during training\n",
    "VALIDATION_PERIOD = 10\n",
    "\n",
    "# Define the maximum number of epochs to run before stopping even if the stopping criteria is not met\n",
    "MAX_EPOCH = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_h1, b_h1, w_h2, b_h2, w_out, b_out, total_training_error, total_validation_error, epochs, training_ending_epoch = ann_fit(train_data, train_labels,\n",
    "                                                                                                                       NUM_INPUT, NUM_HIDDEN1_NEURONS,\n",
    "                                                                                                                       NUM_HIDDEN2_NEURONS, NUM_OUTPUT,\n",
    "                                                                                                                       LR, VALIDATION_PERIOD,\n",
    "                                                                                                                       validation_data, validation_labels)\n",
    "### Optionally play a sound when training is done!\n",
    "# from playsound import playsound\n",
    "#playsound('Victory.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the training error and the validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.plot(epochs, total_training_error, range(0, MAX_EPOCH, VALIDATION_PERIOD), total_validation_error)\n",
    "plt.axis([0, training_ending_epoch, 0, max(np.max(total_training_error), np.max(total_validation_error))])\n",
    "plt.legend(['training error', 'validation error'])\n",
    "plt.title('ANN errors vs. Epochs for NUM_HIDDEN1_NEURONS=%d and NUM_HIDDEN2_NEURONS=%d'%(NUM_HIDDEN1_NEURONS, NUM_HIDDEN2_NEURONS))\n",
    "plt.grid(which='both')\n",
    "plt.savefig('ANNerrors_%d_%d.png'%(NUM_HIDDEN1_NEURONS, NUM_HIDDEN2_NEURONS))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since our test set is not labelled, let's measure instead the model's accuracy on the validation data set like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_labels = np.genfromtxt('validation_labels.csv', delimiter=',')\n",
    "\n",
    "_, predicted_validation_labels = predict_batch(validation_data, w_h1, b_h1, w_h2, b_h2, w_out, b_out)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"ANN Classification Accuracy:\",metrics.accuracy_score(validation_labels, predicted_validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the newly trained ANN on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.genfromtxt('test_set.csv', delimiter=',')\n",
    "\n",
    "_, predicted_labels = predict_batch(test_data, w_h1, b_h1, w_h2, b_h2, w_out, b_out)\n",
    "\n",
    "## Commented-out to avoid overwriting the best result!\n",
    "#np.savetxt('predicted_ann.csv', predicted_labels, delimiter=',\\n', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape, predicted_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification via SVM Algorithm\n",
    "Compiler: Python 3.6.5 <br>\n",
    "OS: Windows 7<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('training_set.csv', delimiter=',')\n",
    "train_labels = np.genfromtxt('training_labels.csv', delimiter=',')\n",
    "\n",
    "validation_data = np.genfromtxt('validation_set.csv', delimiter=',')\n",
    "validation_labels = np.genfromtxt('validation_labels.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = svm.SVC(kernel='linear')\n",
    "train_time_start = time.time()\n",
    "svm_classifier.fit(train_data, train_labels)\n",
    "train_time_end = time.time()\n",
    "\n",
    "print('SVM Training Duration: %3.3f s'%(train_time_end - train_time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_validation_labels = svm_classifier.predict(validation_data)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"SVM Classification Accuracy:\",metrics.accuracy_score(validation_labels, predicted_validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_svm = svm_classifier.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commented-out to avoide overwriting the submitted csv\n",
    "#np.savetxt('predicted_svm.csv', predicted_labels_svm, delimiter=',\\n', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
